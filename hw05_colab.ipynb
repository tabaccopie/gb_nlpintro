{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gb_nlpintro_hw05.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Тема “POS-tagger и NER”\n",
        "\n",
        "####Задание 1. \n",
        "Написать теггер на данных с русским языком\n",
        "- проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации\n",
        "- написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
        "- сравнить все реализованные методы сделать выводы\n",
        " \n",
        "####Задание 2. \n",
        "- Проверить насколько хорошо работает NER\n",
        "данные брать из http://www.labinform.ru/pub/named_entities/\n",
        "- проверить NER из nltk/spacy/deeppavlov\n",
        "- написать свой нер попробовать разные подходы\n",
        "  - передаём в сетку токен и его соседей\n",
        "  - передаём в сетку только токен\n",
        "  - свой вариант\n",
        "- сравнить ваши реализованные подходы на качество (вывести precision/recall/f1_score)\n"
      ],
      "metadata": {
        "id": "seU89fOPKFoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#для проверки теггеров воспользуемся размеченными вручную данными.\n",
        "#такой результат будет более релевантен.\n",
        "\n",
        "!wget -O ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
        "!wget -O ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
      ],
      "metadata": {
        "id": "QfIgb7SKKns1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78f2d1f-6f37-41c9-ceea-4438120b4bb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-15 06:42:42--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40736565 (39M) [text/plain]\n",
            "Saving to: ‘ru_syntagrus-ud-train.conllu’\n",
            "\n",
            "ru_syntagrus-ud-tra 100%[===================>]  38.85M   255MB/s    in 0.2s    \n",
            "\n",
            "2022-07-15 06:42:45 (255 MB/s) - ‘ru_syntagrus-ud-train.conllu’ saved [40736565/40736565]\n",
            "\n",
            "--2022-07-15 06:42:45--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14704579 (14M) [text/plain]\n",
            "Saving to: ‘ru_syntagrus-ud-dev.conllu’\n",
            "\n",
            "ru_syntagrus-ud-dev 100%[===================>]  14.02M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-07-15 06:42:46 (108 MB/s) - ‘ru_syntagrus-ud-dev.conllu’ saved [14704579/14704579]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyconll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRKkriBwR9Db",
        "outputId": "87c7266b-fd00-4f36-84b6-ee22ff1b0401"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyconll in /usr/local/lib/python3.7/dist-packages (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pyconll\n",
        "import nltk\n",
        "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Zsfsy3_bRsQA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pyconll.load_from_file('ru_syntagrus-ud-train.conllu')\n",
        "data_test = pyconll.load_from_file('ru_syntagrus-ud-dev.conllu')"
      ],
      "metadata": {
        "id": "HC6DSjScR7WN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdata_train = []\n",
        "for sent in data_train[:]:\n",
        "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_test = []\n",
        "for sent in data_test[:]:\n",
        "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_sent_test = []\n",
        "for sent in data_test[:]:\n",
        "    fdata_sent_test.append([token.form for token in sent])"
      ],
      "metadata": {
        "id": "7OJ2yg7PSkT2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(fdata_train), len(fdata_test), len(fdata_sent_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSTpYP3FSqCh",
        "outputId": "4fc6c0f0-01d3-410b-89aa-16950f6b18d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24516, 8906, 8906)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdata_train[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC7a9DD2Swn_",
        "outputId": "0d991917-d060-421a-c3d5-3217260db003"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Анкета', 'NOUN'), ('.', 'PUNCT')],\n",
              " [('Начальник', 'NOUN'),\n",
              "  ('областного', 'ADJ'),\n",
              "  ('управления', 'NOUN'),\n",
              "  ('связи', 'NOUN'),\n",
              "  ('Семен', 'PROPN'),\n",
              "  ('Еремеевич', 'PROPN'),\n",
              "  ('был', 'AUX'),\n",
              "  ('человек', 'NOUN'),\n",
              "  ('простой', 'ADJ'),\n",
              "  (',', 'PUNCT'),\n",
              "  ('приходил', 'VERB'),\n",
              "  ('на', 'ADP'),\n",
              "  ('работу', 'NOUN'),\n",
              "  ('всегда', 'ADV'),\n",
              "  ('вовремя', 'ADV'),\n",
              "  (',', 'PUNCT'),\n",
              "  ('здоровался', 'VERB'),\n",
              "  ('с', 'ADP'),\n",
              "  ('секретаршей', 'NOUN'),\n",
              "  ('за', 'ADP'),\n",
              "  ('руку', 'NOUN'),\n",
              "  ('и', 'CCONJ'),\n",
              "  ('иногда', 'ADV'),\n",
              "  ('даже', 'PART'),\n",
              "  ('писал', 'VERB'),\n",
              "  ('в', 'ADP'),\n",
              "  ('стенгазету', 'NOUN'),\n",
              "  ('заметки', 'NOUN'),\n",
              "  ('под', 'ADP'),\n",
              "  ('псевдонимом', 'NOUN'),\n",
              "  ('\"', 'PUNCT'),\n",
              "  ('Муха', 'NOUN'),\n",
              "  ('\"', 'PUNCT'),\n",
              "  ('.', 'PUNCT')]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "unigram_tagger = UnigramTagger(fdata_train)\n",
        "unigram_acc = unigram_tagger.evaluate(fdata_test)\n",
        "\n",
        "bigram_tagger = BigramTagger(fdata_train)\n",
        "bigram_acc = bigram_tagger.evaluate(fdata_test)\n",
        "\n",
        "trigram_tagger = TrigramTagger(fdata_train)\n",
        "trigram_acc = trigram_tagger.evaluate(fdata_test)\n",
        "\n",
        "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
        "bigram_unigram_acc = bigram_tagger.evaluate(fdata_test)\n",
        "\n",
        "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
        "trigram_bigram_unigram_acc = trigram_tagger.evaluate(fdata_test)\n",
        "\n",
        "print(f'Accuracy:\\nUnigram Tagger: {round(unigram_acc, 3)},\\nBigram Tagger: {round(bigram_acc, 5)},\\n'\n",
        "      f'Trigram Tagger: {round(trigram_acc, 3)},\\nBigram and Unigram Tagger: {round(bigram_unigram_acc, 5)},\\n'\n",
        "      f'Trigram, Bigram and Unigram Tagger: {round(trigram_bigram_unigram_acc, 5)},\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ixq-YjS0SzYw",
        "outputId": "4a9738fb-681a-4012-9583-eed2d3a79354"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:\n",
            "Unigram Tagger: 0.824,\n",
            "Bigram Tagger: 0.60939,\n",
            "Trigram Tagger: 0.178,\n",
            "Bigram and Unigram Tagger: 0.82928,\n",
            "Trigram, Bigram and Unigram Tagger: 0.82914,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Комбинация теггеров в данном случае дала некоторый прирост качества"
      ],
      "metadata": {
        "id": "XgnF-FdyUKK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Попробуем написать свой теггер"
      ],
      "metadata": {
        "id": "4EMgcxUMU7HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "whr37IODT5PS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Переведём тренировочный датасет в списки слов и списки POS-разметки\n",
        "train_tok = []\n",
        "train_label = []\n",
        "for sent in fdata_train[:]:\n",
        "    for tok in sent:\n",
        "        train_tok.append(tok[0])\n",
        "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
        "        \n",
        "test_tok = []\n",
        "test_label = []\n",
        "for sent in fdata_test[:]:\n",
        "    for tok in sent:\n",
        "        test_tok.append(' ' if tok[0] is None else tok[0])\n",
        "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
      ],
      "metadata": {
        "id": "4AzVoQzjVGQn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tok[:5], train_label[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7k4O2bhVPlw",
        "outputId": "bc6c1145-abae-4c25-90a4-65d6460efb4f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Анкета', '.', 'Начальник', 'областного', 'управления'],\n",
              " ['NOUN', 'PUNCT', 'NOUN', 'ADJ', 'NOUN'])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "train_enc_labels = le.fit_transform(train_label)\n",
        "train_enc_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV5Yj8isVg_Q",
        "outputId": "62ec21f8-7f2a-45d1-f43d-225d6786f848"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7, 13,  7, ...,  1, 11, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_enc_labels = le.transform(test_label)\n",
        "test_enc_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4lEcTHcV-4s",
        "outputId": "19fa7e25-6bcf-47af-d9ea-fec1cfd87e45"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7, 13,  1, ...,  0,  7, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mySeQ86WCc0",
        "outputId": "4cec183b-b174-4ba0-e41b-569b08e42773"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
              "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
              "       'VERB', 'X'], dtype='<U6')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "vectorizers = [CountVectorizer(ngram_range=(1, 3), analyzer='char'), \n",
        "               TfidfVectorizer(ngram_range=(1, 3), analyzer='char'), \n",
        "               HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=1000)] \n",
        "vectorizers_word = [CountVectorizer(ngram_range=(1, 3), analyzer='word'), \n",
        "               TfidfVectorizer(ngram_range=(1, 3), analyzer='word'), \n",
        "               HashingVectorizer(ngram_range=(1, 3), analyzer='word', n_features=1000)] \n",
        "n_features = [2000, 3000, 5000]\n",
        "vectorizers_hash = [HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=feat) for feat in n_features]\n",
        "vectorizers_hash_word = [HashingVectorizer(ngram_range=(1, 3), analyzer='word', n_features=feat) for feat in n_features]\n",
        "f1_scores = []\n",
        "accuracy_scores = []\n",
        "\n",
        "for vectorizer in vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word:\n",
        "    X_train = vectorizer.fit_transform(train_tok)\n",
        "    X_test = vectorizer.transform(test_tok)\n",
        "    \n",
        "    lr = LogisticRegression(random_state=0, max_iter=100)\n",
        "    lr.fit(X_train, train_enc_labels)\n",
        "    pred = lr.predict(X_test)\n",
        "    f1 = f1_score(test_enc_labels, pred, average='weighted')\n",
        "    f1_scores.append(f1)\n",
        "    acc = accuracy_score(test_enc_labels, pred)\n",
        "    accuracy_scores.append(acc)\n",
        "    \n",
        "    print(vectorizer)\n",
        "    print(classification_report(test_enc_labels, pred, target_names=le.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsusjMCoaMdg",
        "outputId": "e038b272-7149-4dac-ce63-c58e52f2010c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
            "Wall time: 8.11 µs\n",
            "CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.91      0.91      0.91     15103\n",
            "         ADP       0.98      1.00      0.99     13717\n",
            "         ADV       0.90      0.90      0.90      7783\n",
            "         AUX       0.81      0.97      0.88      1390\n",
            "       CCONJ       0.88      0.98      0.93      5672\n",
            "         DET       0.89      0.73      0.80      4265\n",
            "        INTJ       0.35      0.29      0.32        24\n",
            "        NOUN       0.92      0.95      0.93     36238\n",
            "      NO_TAG       1.00      0.77      0.87       265\n",
            "         NUM       0.84      0.90      0.87      1734\n",
            "        PART       0.95      0.76      0.85      5125\n",
            "        PRON       0.83      0.90      0.86      7444\n",
            "       PROPN       0.75      0.58      0.66      5473\n",
            "       PUNCT       1.00      1.00      1.00     29186\n",
            "       SCONJ       0.80      0.91      0.85      2865\n",
            "         SYM       1.00      0.73      0.84        62\n",
            "        VERB       0.94      0.94      0.94     17110\n",
            "           X       0.51      0.15      0.23       134\n",
            "\n",
            "    accuracy                           0.93    153590\n",
            "   macro avg       0.85      0.80      0.81    153590\n",
            "weighted avg       0.93      0.93      0.92    153590\n",
            "\n",
            "TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.89      0.90      0.90     15103\n",
            "         ADP       0.99      0.99      0.99     13717\n",
            "         ADV       0.90      0.86      0.88      7783\n",
            "         AUX       0.82      0.97      0.89      1390\n",
            "       CCONJ       0.89      0.98      0.93      5672\n",
            "         DET       0.82      0.81      0.81      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.90      0.95      0.92     36238\n",
            "      NO_TAG       1.00      0.77      0.87       265\n",
            "         NUM       0.84      0.89      0.86      1734\n",
            "        PART       0.95      0.77      0.85      5125\n",
            "        PRON       0.88      0.85      0.86      7444\n",
            "       PROPN       0.77      0.51      0.62      5473\n",
            "       PUNCT       1.00      1.00      1.00     29186\n",
            "       SCONJ       0.81      0.91      0.86      2865\n",
            "         SYM       1.00      0.69      0.82        62\n",
            "        VERB       0.93      0.92      0.93     17110\n",
            "           X       0.48      0.07      0.13       134\n",
            "\n",
            "    accuracy                           0.92    153590\n",
            "   macro avg       0.82      0.77      0.78    153590\n",
            "weighted avg       0.92      0.92      0.92    153590\n",
            "\n",
            "HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.83      0.83      0.83     15103\n",
            "         ADP       0.97      0.98      0.98     13717\n",
            "         ADV       0.81      0.78      0.79      7783\n",
            "         AUX       0.81      0.96      0.88      1390\n",
            "       CCONJ       0.88      0.97      0.93      5672\n",
            "         DET       0.81      0.77      0.79      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.83      0.90      0.86     36238\n",
            "      NO_TAG       1.00      0.77      0.87       265\n",
            "         NUM       0.80      0.81      0.80      1734\n",
            "        PART       0.92      0.76      0.83      5125\n",
            "        PRON       0.84      0.87      0.85      7444\n",
            "       PROPN       0.69      0.44      0.53      5473\n",
            "       PUNCT       1.00      1.00      1.00     29186\n",
            "       SCONJ       0.80      0.90      0.85      2865\n",
            "         SYM       1.00      0.69      0.82        62\n",
            "        VERB       0.88      0.83      0.85     17110\n",
            "           X       0.31      0.04      0.07       134\n",
            "\n",
            "    accuracy                           0.88    153590\n",
            "   macro avg       0.79      0.74      0.75    153590\n",
            "weighted avg       0.88      0.88      0.88    153590\n",
            "\n",
            "CountVectorizer(ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.93      0.35      0.50     15103\n",
            "         ADP       0.99      0.48      0.64     13717\n",
            "         ADV       0.91      0.77      0.83      7783\n",
            "         AUX       0.84      0.87      0.85      1390\n",
            "       CCONJ       0.89      0.20      0.33      5672\n",
            "         DET       0.84      0.65      0.74      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.98      0.65      0.78     36238\n",
            "      NO_TAG       0.00      0.00      0.00       265\n",
            "         NUM       0.87      0.54      0.67      1734\n",
            "        PART       0.97      0.73      0.83      5125\n",
            "        PRON       0.81      0.80      0.81      7444\n",
            "       PROPN       0.93      0.15      0.25      5473\n",
            "       PUNCT       0.36      1.00      0.53     29186\n",
            "       SCONJ       0.78      0.78      0.78      2865\n",
            "         SYM       0.00      0.00      0.00        62\n",
            "        VERB       0.97      0.41      0.58     17110\n",
            "           X       0.00      0.00      0.00       134\n",
            "\n",
            "    accuracy                           0.63    153590\n",
            "   macro avg       0.67      0.47      0.51    153590\n",
            "weighted avg       0.83      0.63      0.64    153590\n",
            "\n",
            "TfidfVectorizer(ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.93      0.41      0.57     15103\n",
            "         ADP       0.99      0.48      0.64     13717\n",
            "         ADV       0.94      0.78      0.85      7783\n",
            "         AUX       0.84      0.87      0.85      1390\n",
            "       CCONJ       0.88      0.20      0.33      5672\n",
            "         DET       0.76      0.81      0.79      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.98      0.65      0.78     36238\n",
            "      NO_TAG       0.00      0.00      0.00       265\n",
            "         NUM       0.89      0.53      0.66      1734\n",
            "        PART       0.97      0.73      0.83      5125\n",
            "        PRON       0.88      0.75      0.81      7444\n",
            "       PROPN       0.93      0.15      0.25      5473\n",
            "       PUNCT       0.37      1.00      0.54     29186\n",
            "       SCONJ       0.78      0.85      0.82      2865\n",
            "         SYM       0.00      0.00      0.00        62\n",
            "        VERB       0.97      0.47      0.63     17110\n",
            "           X       0.00      0.00      0.00       134\n",
            "\n",
            "    accuracy                           0.64    153590\n",
            "   macro avg       0.67      0.48      0.52    153590\n",
            "weighted avg       0.83      0.64      0.65    153590\n",
            "\n",
            "HashingVectorizer(n_features=1000, ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.41      0.20      0.27     15103\n",
            "         ADP       0.83      0.47      0.60     13717\n",
            "         ADV       0.56      0.62      0.59      7783\n",
            "         AUX       0.70      0.94      0.80      1390\n",
            "       CCONJ       0.84      0.18      0.30      5672\n",
            "         DET       0.50      0.52      0.51      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.25      0.53      0.34     36238\n",
            "      NO_TAG       0.00      0.00      0.00       265\n",
            "         NUM       0.39      0.43      0.41      1734\n",
            "        PART       0.85      0.74      0.79      5125\n",
            "        PRON       0.64      0.74      0.69      7444\n",
            "       PROPN       0.29      0.08      0.12      5473\n",
            "       PUNCT       0.00      0.00      0.00     29186\n",
            "       SCONJ       0.67      0.95      0.78      2865\n",
            "         SYM       0.00      0.00      0.00        62\n",
            "        VERB       0.45      0.24      0.32     17110\n",
            "           X       0.00      0.00      0.00       134\n",
            "\n",
            "    accuracy                           0.36    153590\n",
            "   macro avg       0.41      0.37      0.36    153590\n",
            "weighted avg       0.39      0.36      0.34    153590\n",
            "\n",
            "HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.86      0.86      0.86     15103\n",
            "         ADP       0.97      0.99      0.98     13717\n",
            "         ADV       0.84      0.81      0.82      7783\n",
            "         AUX       0.81      0.97      0.88      1390\n",
            "       CCONJ       0.88      0.97      0.93      5672\n",
            "         DET       0.86      0.73      0.79      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.86      0.92      0.89     36238\n",
            "      NO_TAG       1.00      0.77      0.87       265\n",
            "         NUM       0.81      0.83      0.82      1734\n",
            "        PART       0.93      0.76      0.84      5125\n",
            "        PRON       0.83      0.89      0.86      7444\n",
            "       PROPN       0.71      0.41      0.52      5473\n",
            "       PUNCT       1.00      1.00      1.00     29186\n",
            "       SCONJ       0.81      0.90      0.85      2865\n",
            "         SYM       1.00      0.69      0.82        62\n",
            "        VERB       0.89      0.87      0.88     17110\n",
            "           X       0.38      0.04      0.08       134\n",
            "\n",
            "    accuracy                           0.89    153590\n",
            "   macro avg       0.80      0.75      0.76    153590\n",
            "weighted avg       0.89      0.89      0.89    153590\n",
            "\n",
            "HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.86      0.86      0.86     15103\n",
            "         ADP       0.98      0.98      0.98     13717\n",
            "         ADV       0.86      0.81      0.84      7783\n",
            "         AUX       0.81      0.96      0.88      1390\n",
            "       CCONJ       0.88      0.98      0.93      5672\n",
            "         DET       0.83      0.77      0.80      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.85      0.93      0.89     36238\n",
            "      NO_TAG       1.00      0.77      0.87       265\n",
            "         NUM       0.81      0.83      0.82      1734\n",
            "        PART       0.95      0.76      0.84      5125\n",
            "        PRON       0.84      0.88      0.86      7444\n",
            "       PROPN       0.71      0.41      0.52      5473\n",
            "       PUNCT       1.00      1.00      1.00     29186\n",
            "       SCONJ       0.81      0.90      0.85      2865\n",
            "         SYM       1.00      0.82      0.90        62\n",
            "        VERB       0.90      0.86      0.88     17110\n",
            "           X       0.27      0.07      0.12       134\n",
            "\n",
            "    accuracy                           0.90    153590\n",
            "   macro avg       0.80      0.76      0.77    153590\n",
            "weighted avg       0.89      0.90      0.89    153590\n",
            "\n",
            "HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.87      0.87      0.87     15103\n",
            "         ADP       0.97      0.99      0.98     13717\n",
            "         ADV       0.87      0.82      0.84      7783\n",
            "         AUX       0.81      0.96      0.88      1390\n",
            "       CCONJ       0.89      0.97      0.93      5672\n",
            "         DET       0.84      0.76      0.80      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.87      0.93      0.90     36238\n",
            "      NO_TAG       1.00      0.77      0.87       265\n",
            "         NUM       0.83      0.85      0.84      1734\n",
            "        PART       0.93      0.76      0.84      5125\n",
            "        PRON       0.83      0.87      0.85      7444\n",
            "       PROPN       0.73      0.42      0.54      5473\n",
            "       PUNCT       1.00      1.00      1.00     29186\n",
            "       SCONJ       0.81      0.90      0.85      2865\n",
            "         SYM       1.00      0.69      0.82        62\n",
            "        VERB       0.89      0.89      0.89     17110\n",
            "           X       0.55      0.04      0.08       134\n",
            "\n",
            "    accuracy                           0.90    153590\n",
            "   macro avg       0.82      0.75      0.77    153590\n",
            "weighted avg       0.90      0.90      0.90    153590\n",
            "\n",
            "HashingVectorizer(n_features=2000, ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.48      0.24      0.32     15103\n",
            "         ADP       0.89      0.47      0.62     13717\n",
            "         ADV       0.66      0.67      0.66      7783\n",
            "         AUX       0.75      0.94      0.84      1390\n",
            "       CCONJ       0.89      0.18      0.31      5672\n",
            "         DET       0.67      0.49      0.57      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.60      0.57      0.59     36238\n",
            "      NO_TAG       0.00      0.00      0.00       265\n",
            "         NUM       0.51      0.48      0.49      1734\n",
            "        PART       0.91      0.74      0.82      5125\n",
            "        PRON       0.68      0.83      0.75      7444\n",
            "       PROPN       0.37      0.09      0.15      5473\n",
            "       PUNCT       0.47      1.00      0.64     29186\n",
            "       SCONJ       0.76      0.90      0.82      2865\n",
            "         SYM       0.00      0.00      0.00        62\n",
            "        VERB       0.54      0.29      0.38     17110\n",
            "           X       0.00      0.00      0.00       134\n",
            "\n",
            "    accuracy                           0.58    153590\n",
            "   macro avg       0.51      0.44      0.44    153590\n",
            "weighted avg       0.61      0.58      0.55    153590\n",
            "\n",
            "HashingVectorizer(n_features=3000, ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.53      0.28      0.36     15103\n",
            "         ADP       0.92      0.47      0.62     13717\n",
            "         ADV       0.75      0.70      0.72      7783\n",
            "         AUX       0.77      0.94      0.85      1390\n",
            "       CCONJ       0.93      0.18      0.30      5672\n",
            "         DET       0.71      0.53      0.61      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.64      0.59      0.61     36238\n",
            "      NO_TAG       0.00      0.00      0.00       265\n",
            "         NUM       0.59      0.50      0.54      1734\n",
            "        PART       0.90      0.76      0.82      5125\n",
            "        PRON       0.74      0.81      0.78      7444\n",
            "       PROPN       0.42      0.12      0.18      5473\n",
            "       PUNCT       0.46      1.00      0.63     29186\n",
            "       SCONJ       0.73      0.95      0.82      2865\n",
            "         SYM       0.00      0.00      0.00        62\n",
            "        VERB       0.59      0.32      0.42     17110\n",
            "           X       0.00      0.00      0.00       134\n",
            "\n",
            "    accuracy                           0.59    153590\n",
            "   macro avg       0.54      0.45      0.46    153590\n",
            "weighted avg       0.64      0.59      0.57    153590\n",
            "\n",
            "HashingVectorizer(n_features=5000, ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.60      0.31      0.41     15103\n",
            "         ADP       0.92      0.48      0.63     13717\n",
            "         ADV       0.80      0.71      0.75      7783\n",
            "         AUX       0.79      0.95      0.86      1390\n",
            "       CCONJ       0.91      0.19      0.31      5672\n",
            "         DET       0.70      0.67      0.69      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.69      0.61      0.64     36238\n",
            "      NO_TAG       0.00      0.00      0.00       265\n",
            "         NUM       0.63      0.49      0.55      1734\n",
            "        PART       0.91      0.74      0.82      5125\n",
            "        PRON       0.79      0.78      0.78      7444\n",
            "       PROPN       0.52      0.14      0.22      5473\n",
            "       PUNCT       0.44      1.00      0.61     29186\n",
            "       SCONJ       0.75      0.88      0.81      2865\n",
            "         SYM       0.00      0.00      0.00        62\n",
            "        VERB       0.66      0.35      0.46     17110\n",
            "           X       0.00      0.00      0.00       134\n",
            "\n",
            "    accuracy                           0.60    153590\n",
            "   macro avg       0.56      0.46      0.47    153590\n",
            "weighted avg       0.67      0.60      0.59    153590\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_model = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word,\n",
        "                            'f1_score': f1_scores})\n",
        "result_model.sort_values('f1_score', ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "3TdVDBqYlPnm",
        "outputId": "63f2e894-3d74-4856-e410-50732ff7075e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           Vectorizer  f1_score\n",
              "0   CountVectorizer(analyzer='char', ngram_range=(...  0.924048\n",
              "1   TfidfVectorizer(analyzer='char', ngram_range=(...  0.917500\n",
              "8   HashingVectorizer(analyzer='char', n_features=...  0.897982\n",
              "7   HashingVectorizer(analyzer='char', n_features=...  0.893299\n",
              "6   HashingVectorizer(analyzer='char', n_features=...  0.890910\n",
              "2   HashingVectorizer(analyzer='char', n_features=...  0.876592\n",
              "4                 TfidfVectorizer(ngram_range=(1, 3))  0.654762\n",
              "3                 CountVectorizer(ngram_range=(1, 3))  0.637163\n",
              "11  HashingVectorizer(n_features=5000, ngram_range...  0.585894\n",
              "10  HashingVectorizer(n_features=3000, ngram_range...  0.568290\n",
              "9   HashingVectorizer(n_features=2000, ngram_range...  0.547066\n",
              "5   HashingVectorizer(n_features=1000, ngram_range...  0.340580"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-10cfb8df-abec-420a-9f7d-0df062a37ecb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Vectorizer</th>\n",
              "      <th>f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CountVectorizer(analyzer='char', ngram_range=(...</td>\n",
              "      <td>0.924048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TfidfVectorizer(analyzer='char', ngram_range=(...</td>\n",
              "      <td>0.917500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
              "      <td>0.897982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
              "      <td>0.893299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
              "      <td>0.890910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
              "      <td>0.876592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TfidfVectorizer(ngram_range=(1, 3))</td>\n",
              "      <td>0.654762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CountVectorizer(ngram_range=(1, 3))</td>\n",
              "      <td>0.637163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>HashingVectorizer(n_features=5000, ngram_range...</td>\n",
              "      <td>0.585894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>HashingVectorizer(n_features=3000, ngram_range...</td>\n",
              "      <td>0.568290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>HashingVectorizer(n_features=2000, ngram_range...</td>\n",
              "      <td>0.547066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>HashingVectorizer(n_features=1000, ngram_range...</td>\n",
              "      <td>0.340580</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10cfb8df-abec-420a-9f7d-0df062a37ecb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-10cfb8df-abec-420a-9f7d-0df062a37ecb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-10cfb8df-abec-420a-9f7d-0df062a37ecb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_model_acc = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word,\n",
        "                            'Accuracy': accuracy_scores})\n",
        "result_model_acc.sort_values('Accuracy', ascending=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "nJB392XJlZ_5",
        "outputId": "af255e62-bea7-47b1-ab42-c683cc6534ec"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           Vectorizer  Accuracy\n",
              "0   CountVectorizer(analyzer='char', ngram_range=(...  0.925802\n",
              "1   TfidfVectorizer(analyzer='char', ngram_range=(...  0.919917\n",
              "8   HashingVectorizer(analyzer='char', n_features=...  0.901543\n",
              "7   HashingVectorizer(analyzer='char', n_features=...  0.896771\n",
              "6   HashingVectorizer(analyzer='char', n_features=...  0.894511\n",
              "2   HashingVectorizer(analyzer='char', n_features=...  0.879510\n",
              "4                 TfidfVectorizer(ngram_range=(1, 3))  0.644176\n",
              "3                 CountVectorizer(ngram_range=(1, 3))  0.628068\n",
              "11  HashingVectorizer(n_features=5000, ngram_range...  0.604213\n",
              "10  HashingVectorizer(n_features=3000, ngram_range...  0.592786\n",
              "9   HashingVectorizer(n_features=2000, ngram_range...  0.575812\n",
              "5   HashingVectorizer(n_features=1000, ngram_range...  0.359861"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55e666e4-ad94-4c09-9e93-87ba60f012be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Vectorizer</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CountVectorizer(analyzer='char', ngram_range=(...</td>\n",
              "      <td>0.925802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TfidfVectorizer(analyzer='char', ngram_range=(...</td>\n",
              "      <td>0.919917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
              "      <td>0.901543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
              "      <td>0.896771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
              "      <td>0.894511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
              "      <td>0.879510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TfidfVectorizer(ngram_range=(1, 3))</td>\n",
              "      <td>0.644176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CountVectorizer(ngram_range=(1, 3))</td>\n",
              "      <td>0.628068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>HashingVectorizer(n_features=5000, ngram_range...</td>\n",
              "      <td>0.604213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>HashingVectorizer(n_features=3000, ngram_range...</td>\n",
              "      <td>0.592786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>HashingVectorizer(n_features=2000, ngram_range...</td>\n",
              "      <td>0.575812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>HashingVectorizer(n_features=1000, ngram_range...</td>\n",
              "      <td>0.359861</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55e666e4-ad94-4c09-9e93-87ba60f012be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-55e666e4-ad94-4c09-9e93-87ba60f012be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-55e666e4-ad94-4c09-9e93-87ba60f012be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Задание 2"
      ],
      "metadata": {
        "id": "yX6xsNkTqK1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install corus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhn-uJD-qNpv",
        "outputId": "53c18840-2158-4990-e319-16c5b1f2b469"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting corus\n",
            "  Downloading corus-0.9.0-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: corus\n",
            "Successfully installed corus-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import corus\n",
        "from corus import load_ne5\n",
        "import nltk"
      ],
      "metadata": {
        "id": "28r1Kky5qPwJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGrglS6LqWcg",
        "outputId": "f655a136-5a74-43d5-9156-a57341767660"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.labinform.ru/pub/named_entities/collection5.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNWsa4qpqafO",
        "outputId": "97c7e9b2-bbfe-41f3-e363-e7c129c9251d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-15 07:01:54--  http://www.labinform.ru/pub/named_entities/collection5.zip\n",
            "Resolving www.labinform.ru (www.labinform.ru)... 95.181.230.181\n",
            "Connecting to www.labinform.ru (www.labinform.ru)|95.181.230.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1899530 (1.8M) [application/zip]\n",
            "Saving to: ‘collection5.zip’\n",
            "\n",
            "collection5.zip     100%[===================>]   1.81M  1.50MB/s    in 1.2s    \n",
            "\n",
            "2022-07-15 07:01:56 (1.50 MB/s) - ‘collection5.zip’ saved [1899530/1899530]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip collection5.zip"
      ],
      "metadata": {
        "id": "-s9lGFgAqg1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records = load_ne5('Collection5/')\n",
        "document = next(records).text\n",
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "FtjyQP98qmp6",
        "outputId": "d746a5fc-9c02-4923-ee1a-482e1fd02792"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Президент России уволил 6 генералов МВД\\r\\n \\r\\nПрезидент России Дмитрий Медведев продолжает перестановки в руководстве МВД. Сегодня он уволил 6 генералов милиции.\\r\\n\\r\\nВ частности, указом президента отправлен в отставку начальник Центра обеспечения оперативно-служебной деятельности по противодействию экстремизму МВД генерал-майор милиции Владимир Булатов. Не взяли на работу в полицию и ряд региональных руководителей ведомства. Уволены начальник Управления на транспорте МВД по Северо-Кавказскому федеральному округу генерал-майор милиции Игорь Жуков, начальник Управления внутренних дел по Тверской области генерал-майор милиции Александр Куликов, начальник Управления внутренних дел по Тамбовской области генерал-майор милиции Владимир Фурсов.\\r\\n\\r\\nТакже президент отправил в отставку начальников 2 милицейских институтов: начальника Сибирского юридического института МВД РФ генерал-майора милиции Николая Михайлова и начальника Тюменского юридического института МВД генерал-майора милиции Александра Числова.\\r\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK_JKuykqzWO",
        "outputId": "20c73aa5-08ce-4385-cb92-2f24720d0ad7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('Николая Михайлова', 'PERSON'),\n",
              " ('Президент', 'PERSON'),\n",
              " ('России', 'ORGANIZATION'),\n",
              " ('России Дмитрий Медведев', 'PERSON'),\n",
              " ('Сибирского', 'ORGANIZATION'),\n",
              " ('Управления', 'ORGANIZATION'),\n",
              " ('Центра', 'PERSON')}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В целом распознано верно, но есть погрешности"
      ],
      "metadata": {
        "id": "RzgaPKjIq7X1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Spacy"
      ],
      "metadata": {
        "id": "L0zoyCL4rg1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIsMS559rsZ8",
        "outputId": "6f2d00ea-60a0-49f8-8f72-bda396dc7861"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w--ffTlr47R",
        "outputId": "ba260da3-b39b-43d0-b54c-549b55544848"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-07-15 07:08:25.389093: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ru-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.4.0/ru_core_news_sm-3.4.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from ru-core-news-sm==3.4.0) (3.4.0)\n",
            "Collecting pymorphy2>=0.9\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2>=0.9->ru-core-news-sm==3.4.0) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 11.5 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.8.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.0.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2, ru-core-news-sm\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 ru-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import ru_core_news_sm"
      ],
      "metadata": {
        "id": "0rMeJ-8QsGLJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = ru_core_news_sm.load()\n",
        "ny_bb = document\n",
        "article = nlp(ny_bb)\n",
        "displacy.render(article, jupyter=True, style='ent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "vxtAtCdXrBpX",
        "outputId": "04661741-618c-4875-b4c9-430d2c45c914"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Президент \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    России\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " уволил 6 генералов \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    МВД\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "\r</br> \r</br>Президент \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    России\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Дмитрий Медведев\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " продолжает перестановки в руководстве \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    МВД\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ". Сегодня он уволил 6 генералов милиции.\r</br>\r</br>В частности, указом президента отправлен в отставку начальник \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Центра обеспечения оперативно-служебной деятельности по противодействию экстремизму\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    МВД\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " генерал-майор милиции \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Владимир Булатов\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ". Не взяли на работу в полицию и ряд региональных руководителей ведомства. Уволены начальник \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Управления на транспорте\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    МВД\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " по \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Северо-Кавказскому федеральному округу\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " генерал-майор милиции \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Игорь Жуков\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ", начальник \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Управления внутренних дел\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " по \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Тверской области\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " генерал-майор милиции \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Александр Куликов\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ", начальник \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Управления внутренних дел\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " по \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Тамбовской области\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " генерал-майор милиции \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Владимир Фурсов\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ".\r</br>\r</br>Также президент отправил в отставку начальников 2 милицейских институтов: начальника \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Сибирского юридического института\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    МВД\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    РФ\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " генерал-майора милиции \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Николая Михайлова\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " и начальника \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Тюменского юридического института\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    МВД\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " генерал-майора милиции \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Александра Числова\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ".\r</br></div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#список токенов, частей речи и сущностей\n",
        "for token in article:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naEr6Yu2svnm",
        "outputId": "5f9bf1e5-e4fa-494a-e471-332c1cb89f2f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Президент NOUN nsubj\n",
            "России PROPN nmod\n",
            "уволил VERB ROOT\n",
            "6 NUM nummod\n",
            "генералов NOUN obj\n",
            "МВД PROPN nmod\n",
            "\r\n",
            " \r\n",
            " SPACE dep\n",
            "Президент NOUN nsubj\n",
            "России PROPN nmod\n",
            "Дмитрий PROPN appos\n",
            "Медведев PROPN flat:name\n",
            "продолжает VERB conj\n",
            "перестановки NOUN obj\n",
            "в ADP case\n",
            "руководстве NOUN nmod\n",
            "МВД PROPN nmod\n",
            ". PUNCT punct\n",
            "Сегодня ADV advmod\n",
            "он PRON nsubj\n",
            "уволил VERB ROOT\n",
            "6 NUM nummod\n",
            "генералов NOUN obj\n",
            "милиции NOUN nmod\n",
            ". PUNCT punct\n",
            "\r\n",
            "\r\n",
            " SPACE dep\n",
            "В ADP discourse\n",
            "частности NOUN fixed\n",
            ", PUNCT punct\n",
            "указом NOUN obl\n",
            "президента NOUN nmod\n",
            "отправлен VERB ROOT\n",
            "в ADP case\n",
            "отставку NOUN obl\n",
            "начальник NOUN nsubj:pass\n",
            "Центра PROPN nmod\n",
            "обеспечения NOUN nmod\n",
            "оперативно ADJ amod\n",
            "- ADJ amod\n",
            "служебной ADJ amod\n",
            "деятельности NOUN nmod\n",
            "по ADP case\n",
            "противодействию NOUN nmod\n",
            "экстремизму NOUN iobj\n",
            "МВД PROPN nmod\n",
            "генерал NOUN appos\n",
            "- NOUN appos\n",
            "майор NOUN appos\n",
            "милиции NOUN nmod\n",
            "Владимир PROPN appos\n",
            "Булатов PROPN flat:name\n",
            ". PUNCT punct\n",
            "Не PART advmod\n",
            "взяли VERB ROOT\n",
            "на ADP case\n",
            "работу NOUN obl\n",
            "в ADP case\n",
            "полицию NOUN obl\n",
            "и CCONJ cc\n",
            "ряд NOUN conj\n",
            "региональных ADJ amod\n",
            "руководителей NOUN nmod\n",
            "ведомства NOUN nmod\n",
            ". PUNCT punct\n",
            "Уволены VERB ROOT\n",
            "начальник NOUN nsubj:pass\n",
            "Управления PROPN nmod\n",
            "на ADP case\n",
            "транспорте NOUN nmod\n",
            "МВД PROPN nmod\n",
            "по ADP case\n",
            "Северо ADJ amod\n",
            "- ADJ amod\n",
            "Кавказскому ADJ amod\n",
            "федеральному ADJ amod\n",
            "округу NOUN nmod\n",
            "генерал NOUN appos\n",
            "- NOUN appos\n",
            "майор NOUN appos\n",
            "милиции NOUN nmod\n",
            "Игорь PROPN appos\n",
            "Жуков PROPN flat:name\n",
            ", PUNCT punct\n",
            "начальник NOUN conj\n",
            "Управления PROPN nmod\n",
            "внутренних ADJ amod\n",
            "дел NOUN nmod\n",
            "по ADP case\n",
            "Тверской ADJ amod\n",
            "области NOUN nmod\n",
            "генерал NOUN appos\n",
            "- NOUN appos\n",
            "майор NOUN appos\n",
            "милиции NOUN nmod\n",
            "Александр PROPN appos\n",
            "Куликов PROPN flat:name\n",
            ", PUNCT punct\n",
            "начальник NOUN conj\n",
            "Управления PROPN nmod\n",
            "внутренних ADJ amod\n",
            "дел NOUN nmod\n",
            "по ADP case\n",
            "Тамбовской ADJ amod\n",
            "области NOUN nmod\n",
            "генерал NOUN appos\n",
            "- NOUN appos\n",
            "майор NOUN appos\n",
            "милиции NOUN nmod\n",
            "Владимир PROPN appos\n",
            "Фурсов PROPN flat:name\n",
            ". PUNCT punct\n",
            "\r\n",
            "\r\n",
            " SPACE dep\n",
            "Также ADV advmod\n",
            "президент NOUN nsubj\n",
            "отправил VERB ROOT\n",
            "в ADP case\n",
            "отставку NOUN obl\n",
            "начальников NOUN obj\n",
            "2 NUM nummod\n",
            "милицейских ADJ amod\n",
            "институтов NOUN nmod\n",
            ": PUNCT punct\n",
            "начальника NOUN parataxis\n",
            "Сибирского ADJ amod\n",
            "юридического ADJ amod\n",
            "института NOUN nmod\n",
            "МВД PROPN nmod\n",
            "РФ PROPN nmod\n",
            "генерал NOUN appos\n",
            "- NOUN appos\n",
            "майора NOUN appos\n",
            "милиции NOUN nmod\n",
            "Николая PROPN appos\n",
            "Михайлова PROPN flat:name\n",
            "и CCONJ cc\n",
            "начальника NOUN conj\n",
            "Тюменского ADJ amod\n",
            "юридического ADJ amod\n",
            "института NOUN nmod\n",
            "МВД PROPN nmod\n",
            "генерал NOUN appos\n",
            "- NOUN appos\n",
            "майора NOUN appos\n",
            "милиции NOUN nmod\n",
            "Александра PROPN appos\n",
            "Числова PROPN flat:name\n",
            ". PUNCT punct\n",
            "\r\n",
            " SPACE dep\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DeepPavlov"
      ],
      "metadata": {
        "id": "Y-ai_SzTtC-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.version.VERSION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO0SLrjksyiY",
        "outputId": "a22754a3-e074-4bfe-a17d-9603888afabe"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy scipy librosa unidecode inflect librosa transformers\n",
        "!pip install deeppavlov"
      ],
      "metadata": {
        "id": "MRrcDNJdt08Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m deeppavlov install squad_bert\n",
        "!python -m deeppavlov install ner_ontonotes"
      ],
      "metadata": {
        "id": "ZjpnyGh1uPpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deeppavlov\n",
        "from deeppavlov import configs, build_model"
      ],
      "metadata": {
        "id": "JhrGghvduilT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deeppavlov_ner = build_model(configs.ner.ner_bert_ent_and_type_rus, download=True)\n",
        "deeppavlov_ner(document)"
      ],
      "metadata": {
        "id": "JVitu7Ddupg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### С deeppavlov не удалось разобраться -- много ошибок с зависимостями и видимо надо отдельно погружаться в тему"
      ],
      "metadata": {
        "id": "qQbK3bDVvjKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Свой NER"
      ],
      "metadata": {
        "id": "i-aikfyMv1B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install razdel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Q_p5HhvuAz",
        "outputId": "a55f8fc6-c0c3-41d3-e553-0b21bb81b44c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting razdel\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from razdel import tokenize"
      ],
      "metadata": {
        "id": "PmvUB5rEv-r8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_docs = []\n",
        "for ix, rec in enumerate(records):\n",
        "    words = []\n",
        "    for token in tokenize(rec.text):\n",
        "        \n",
        "        type_ent = 'OUT'\n",
        "        for ent in rec.spans:\n",
        "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
        "                type_ent = ent.type\n",
        "                break\n",
        "        words.append([token.text, type_ent])\n",
        "    words_docs.extend(words)"
      ],
      "metadata": {
        "id": "DJxRsyTZwCNi"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
      ],
      "metadata": {
        "id": "uE6VjNQZwHGi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_words['tag'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpTcg8PzwJGK",
        "outputId": "742a6427-9ed7-4083-e2f9-af119f4fb4bf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OUT         219139\n",
              "PER          21186\n",
              "ORG          13623\n",
              "LOC           4561\n",
              "GEOPOLIT      4353\n",
              "MEDIA         2482\n",
              "Name: tag, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input, Bidirectional,Reshape\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from sklearn import model_selection, preprocessing, linear_model"
      ],
      "metadata": {
        "id": "f3ehLPiMwOG6"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
        "\n",
        "# labelEncode\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "metadata": {
        "id": "GJLvGFSKwSeb"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.apply(len).max(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-3DOpYLwWbf",
        "outputId": "bc7d6191-fc65-4f6d-df30-970c0902f95f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
        "\n",
        "train_data = train_data.batch(2048)\n",
        "valid_data = valid_data.batch(2048)"
      ],
      "metadata": {
        "id": "eb0Y3qv-wZH3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "gRCBpNodwbvp"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "    return input_data\n",
        "\n",
        "vocab_size = 30000\n",
        "seq_len = 10\n",
        "\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=seq_len\n",
        "    )\n",
        "\n",
        "text_data = train_data.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_data)"
      ],
      "metadata": {
        "id": "afojiD8JwecW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=np.unique(encoder.inverse_transform(valid_y),return_counts=True)[1]\n",
        "t=t/t.sum()\n",
        "t  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGjLGD9SwnhI",
        "outputId": "52bde43c-2e96-464e-fdc2-c981cc377e43"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01685359, 0.01748673, 0.00838157, 0.05123915, 0.82578087,\n",
              "       0.08025808])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  tf.keras.layers.Embedding(len(vectorize_layer.get_vocabulary()), 64, mask_zero=True),\n",
        "  tf.keras.layers.GlobalAveragePooling1D(),\n",
        "  tf.keras.layers.Dense(300, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(50, activation='relu'),\n",
        "  tf.keras.layers.Dense(6, activation='softmax')\n",
        "  ])"
      ],
      "metadata": {
        "id": "alAjJmBUwqsG"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',         \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_data, validation_data=valid_data, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC--bCGGwwCa",
        "outputId": "2d024f2b-db72-432b-d3c5-9fd7daacbe6a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "98/98 [==============================] - 8s 69ms/step - loss: 0.7737 - accuracy: 0.8235 - val_loss: 0.3918 - val_accuracy: 0.8683\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 5s 48ms/step - loss: 0.2621 - accuracy: 0.9122 - val_loss: 0.2547 - val_accuracy: 0.9324\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 5s 49ms/step - loss: 0.1484 - accuracy: 0.9559 - val_loss: 0.2377 - val_accuracy: 0.9405\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 5s 54ms/step - loss: 0.1196 - accuracy: 0.9633 - val_loss: 0.2335 - val_accuracy: 0.9414\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 5s 49ms/step - loss: 0.1116 - accuracy: 0.9644 - val_loss: 0.2324 - val_accuracy: 0.9412\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff1e7083410>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred=model.predict(valid_data)"
      ],
      "metadata": {
        "id": "B8SuSNw0w7IU"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(encoder.classes_,pred.mean(axis=0)/t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FepzVpBw_0W",
        "outputId": "eb6f23f3-b21c-40e1-a2dc-d53a98b440b1"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('GEOPOLIT', 0.9981119692965357),\n",
              " ('LOC', 0.9751331507131971),\n",
              " ('MEDIA', 1.522513045949473),\n",
              " ('ORG', 1.3357483595741184),\n",
              " ('OUT', 0.990347254078025),\n",
              " ('PER', 0.83858281333674)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    tf.keras.layers.Embedding(len(vectorize_layer.get_vocabulary()), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(6,activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "qGROsqY_xDCm"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "xGjhhtIMxIIz"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data, validation_data=valid_data, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15c1U3EcxK9T",
        "outputId": "8a379287-2ae4-4b0f-c0a2-a1a68bac6a09"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "98/98 [==============================] - 46s 354ms/step - loss: 0.8406 - accuracy: 0.8156 - val_loss: 0.4179 - val_accuracy: 0.8424\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 32s 323ms/step - loss: 0.3210 - accuracy: 0.8989 - val_loss: 0.2730 - val_accuracy: 0.9178\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 32s 324ms/step - loss: 0.2027 - accuracy: 0.9399 - val_loss: 0.2258 - val_accuracy: 0.9355\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 32s 332ms/step - loss: 0.1549 - accuracy: 0.9555 - val_loss: 0.2103 - val_accuracy: 0.9398\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 31s 321ms/step - loss: 0.1340 - accuracy: 0.9617 - val_loss: 0.2088 - val_accuracy: 0.9408\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff1e4a2bc50>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}