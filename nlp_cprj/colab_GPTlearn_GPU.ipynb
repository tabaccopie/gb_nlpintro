{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Курсовая работа\n",
        "\n",
        "**Этап 1. Обучение модели**"
      ],
      "metadata": {
        "id": "yy1j1avXpwqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers deepspeed --quiet"
      ],
      "metadata": {
        "id": "uMgzQEinp4wh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c17e671b-3823-4a73-c8c7-ba77bce9aa6b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.7 MB 15.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 704 kB 71.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 73.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 46.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 108 kB 67.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.2 MB/s \n",
            "\u001b[?25h  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# запишем в файл и в дальнейшем запустим его -- необходимо из-за краша\n",
        "# среды выполнения в colab\n",
        "%%writefile train_model.py\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# определим переменные для deepspeed\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '9994'\n",
        "os.environ['RANK'] = \"0\"\n",
        "os.environ['LOCAL_RANK'] = \"0\"# for ddp\n",
        "os.environ['WORLD_SIZE'] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\" #uncoment for large files\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM\n",
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        "from transformers import GPT2TokenizerFast\n",
        "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel, PreTrainedTokenizerFast\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "torch.manual_seed(42)\n",
        "MODEL_DIR =  './drive/MyDrive/nlp-project/model/mailqa'\n",
        "\n",
        "device = 'cuda:0'\n",
        "backbone = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(backbone, use_fast=True)\n",
        "\n",
        "train_path = './drive/MyDrive/nlp-project/Otvety.txt'\n",
        "\n",
        "#необходимые функции для токенизации и датасета\n",
        "\n",
        "def tokenize(text):\n",
        "    print(f'Tokenizing text length {len(text)}')\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "\n",
        "# Custom dataset loader using multiprocessing to parallelize tokenization\n",
        "class MailRuDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizerFast, file_path: str, block_size: int = 1024):\n",
        "        if not os.path.exists('./drive/MyDrive/nlp-project/cached_dataset'):\n",
        "            self.samples = []\n",
        "\n",
        "            print('Чтение файла')\n",
        "            with open(file_path, encoding='utf-8', errors='ignore') as data_file:\n",
        "                data = data_file.read()\n",
        "\n",
        "            print(f'Размер данных: {len(data)}')\n",
        "\n",
        "            print('Разбиение файла данных')\n",
        "            data_chunks = [data[i:i+2097152] for i in tqdm(range(0, len(data), 2097152))]\n",
        "\n",
        "            del data\n",
        "            gc.collect()\n",
        "\n",
        "            print(f'Количество частей: {len(data_chunks)}')\n",
        "\n",
        "            print('Старт токенизации')\n",
        "            with Pool(8) as p:\n",
        "                tokenized_text = [token for tokens in p.map(tokenize, data_chunks) for token in tokens]\n",
        "                p.close()\n",
        "                p.join()\n",
        "\n",
        "            del data_chunks\n",
        "            gc.collect()\n",
        "\n",
        "            print(f'Размер токенизированного текста: {len(tokenized_text)}')\n",
        "\n",
        "            print('Разбиение поблочно')\n",
        "            self.samples = [tokenized_text[i:i+block_size] for i in range(0, len(tokenized_text) - block_size + 1, block_size)]\n",
        "\n",
        "            print(f'Число сэмплов: {len(self.samples)}')\n",
        "\n",
        "            del tokenized_text\n",
        "            gc.collect()\n",
        "\n",
        "            pickle.dump(self.samples, open(\"./drive/MyDrive/nlp-project/cached_dataset\", \"wb\"))\n",
        "\n",
        "            print('Датасет загружен и закеширован на диск')\n",
        "        else:\n",
        "            print('Загрузка датасета из кеша')\n",
        "            self.samples = pickle.load(open('./drive/MyDrive/nlp-project/cached_dataset', 'rb'))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.samples[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "\n",
        "def load_dataset(train_path, tokenizer):\n",
        "    train_dataset = MailRuDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=1024)\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "\n",
        "    return train_dataset, data_collator\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_dataset, data_collator = load_dataset(train_path, tokenizer)\n",
        "    \n",
        "    model = GPT2LMHeadModel.from_pretrained(backbone).to(device)\n",
        "\n",
        "    training_args = TrainingArguments(output_dir=MODEL_DIR,\n",
        "                                      num_train_epochs=1, \n",
        "                                      logging_steps=50, \n",
        "                                      save_steps=2000,\n",
        "                                      per_device_train_batch_size=1,\n",
        "                                      per_device_eval_batch_size=1,\n",
        "                                      warmup_steps=100,\n",
        "                                      weight_decay=0.01, \n",
        "                                      fp16=True,\n",
        "                                      report_to=None,\n",
        "                                      save_total_limit=5)\n",
        "    trainer = Trainer(model=model, args=training_args, \n",
        "            data_collator=data_collator,\n",
        "            train_dataset=train_dataset,\n",
        "                      \n",
        "    )\n",
        "\n",
        "    trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smS3drVGtGqT",
        "outputId": "f6f22d16-7324-47a2-e1d7-96e810bf7c57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train_model.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5OMHKasuYLA",
        "outputId": "aaccca0c-cb22-4f66-e0aa-0da75da37a61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading vocab.json: 100% 1.63M/1.63M [00:00<00:00, 3.27MB/s]\n",
            "Downloading merges.txt: 100% 1.21M/1.21M [00:00<00:00, 2.45MB/s]\n",
            "Downloading config.json: 100% 608/608 [00:00<00:00, 610kB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Загрузка датасета из кеша\n",
            "Downloading pytorch_model.bin: 100% 526M/526M [00:08<00:00, 61.6MB/s]\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}